{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the MLflow Client API (pt)Usando o MLflow Client API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we started an instance of the MLflow Tracking Server and the MLflow UI. For this stage, we’re going to be interfacing with the Tracking Server through one of the primary mechanisms that you will use when training ML models, the MlflowClient. For the duration of this tutorial, this client API will be your primary interface for MLflow’s tracking capabilities, enabling you to: \n",
    "\n",
    "(pt)Na seção anterior, iniciamos uma instância do MLflow Tracking Server e da UI do MLflow. Para esta etapa, faremos a interface com o Tracking Server por meio de um dos principais mecanismos que você usará ao treinar modelos de ML, o MlflowClient. Durante este tutorial, esta API cliente será sua interface principal para os recursos de rastreamento do MLflow, permitindo que você:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initiate a new Experiment. (pt)Inicie um novo experimento.\n",
    "* Start Runs within an Experiment. (pt)Iniciar execuções em um experimento.\n",
    "* Document parameters, metrics, and tags for your Runs. (pt)Documente parametros, metricas, e tags para sua execução.\n",
    "* log artifacts linked to runs, such as models, tables, plots, and more. (pt)log artefatos linkados com execuções, assim como os modelos, tabelas, gráficos, e mais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dependencies (pt)Importando dependências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the MLflowClient API, the initial step involves importing the necessary modules. (pt) Na ordem de uso do MLflowClient API, o passo inicial envolve importar os modulos necessários."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "from pprint import pprint\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these modules imported, you’re now prepared to configure the client and relay specifics about the location of your tracking server. (pt)Com esses módulos importados, agora você está preparado para configurar o cliente e transmitir detalhes específicos sobre a localização do seu servidor de rastreamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the MLflow Tracking Client (pt)Configurando o Client de rastreamento MLflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, barring any modifications to the MLFLOW_TRACKING_URI environment variable, initializing the MlflowClient will designate your local storage as the tracking server. This means your experiments, data, models, and related attributes will be stored within the active execution directory. (pt)Por padrão, salvo qualquer modificação nas variáveis de ambiente do MLFLOW_TRACKING_URI, inicializando o MLflowClient irá designar você para seu arquivo local como o servidor de rastreamento.\n",
    "\n",
    "For the context of this guide, we’ll utilize the tracking server initialized earlier in the documentation, instead of using the client to log to the local file system directory.\n",
    "\n",
    "In order to connect to the tracking server that we created in the previous section of this tutorial, we’ll need to use the uri that we assigned the server when we started it. The two components that we submitted as arguments to the mlflow server command were the host and the port. Combined, these form the tracking_uri argument that we will specify to start an instance of the client.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient(tracking_uri=\"http://127.0.0.1:8080\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a client interface to the tracking server that can both send data to and retrieve data from the tracking server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Default Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get to logging anything to the Tracking Server, let’s take a look at a key feature that exists at the outset of starting any MLflow Tracking Server: the Default Experiment.\n",
    "\n",
    "The Default Experiment is a placeholder that is used to encapsulate all run information if an explicit Experiment is not declared. While using MLflow, you’ll be creating new experiments in order to organize projects, project iterations, or logically group large modeling activities together in a grouped hierarchical collection. However, if you manage to forget to create a new Experiment before using the MLflow tracking capabilities, the Default Experiment is a fallback for you to ensure that your valuable tracking data is not lost when executing a run.\n",
    "\n",
    "Let’s see what this Default Experiment looks like by using the mlflow.client.MlflowClient.search_experiments() API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that we’re going to do is to view the metadata associated with the Experiments that are on the server. We can accomplish this through the use of the mlflow.client.MlflowClient.search_experiments() API. Let’s issue a search query to see what the results are.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Experiment: artifact_location='mlflow-artifacts:/665823732664597081', creation_time=1707855087841, experiment_id='665823732664597081', last_update_time=1707855087841, lifecycle_stage='active', name='MLflow Quickstart', tags={}>, <Experiment: artifact_location='mlflow-artifacts:/232886580622657502', creation_time=1707574559267, experiment_id='232886580622657502', last_update_time=1707574559267, lifecycle_stage='active', name='Apple_Models', tags={'mlflow.note.content': 'This is the grocery forecasting project. This '\n",
      "                        'experiment contains the produce models for apples.',\n",
      " 'project_name': 'grocery-forecasting',\n",
      " 'project_quarter': 'Q3-2023',\n",
      " 'store_dept': 'produce',\n",
      " 'team': 'stores-ml'}>, <Experiment: artifact_location='mlflow-artifacts:/0', creation_time=1707574453804, experiment_id='0', last_update_time=1707574453804, lifecycle_stage='active', name='Default', tags={}>]\n"
     ]
    }
   ],
   "source": [
    "all_experiments = client.search_experiments()\n",
    "\n",
    "print(all_experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Output result\n",
    "\n",
    "[<Experiment: artifact_location='./mlruns/0', creation_time=None, experiment_id='0', last_update_time=None, lifecycle_stage='active', name='Default', tags={}>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that the return type of the search_experiments() API is not a basic collection structure. Rather, it is a list of Experiment objects. Many of the return values of MLflow’s client APIs return objects that contain metadata attributes associated with the task being performed. This is an important aspect to remember, as it makes more complex sequences of actions easier to perform, which will be covered in later tutorials.\n",
    "\n",
    "With the returned collection, we can iterate over these objects with a comprehension to access the specific metadata attributes of the Default experiment.\n",
    "\n",
    "To get familiar with accessing elements from returned collections from MLflow APIs, let’s extract the name and the lifecycle_stage from the search_experiments() query and extract these attributes into a dict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Default', 'lifecycle_stage': 'active'}\n"
     ]
    }
   ],
   "source": [
    "default_experiment = [\n",
    "    {\"name\": experiment.name, \"lifecycle_stage\": experiment.lifecycle_stage}\n",
    "    for experiment in all_experiments\n",
    "    if experiment.name == \"Default\"\n",
    "][0]\n",
    "\n",
    "print(default_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default Experiment name and stage as a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'name': 'Default', 'lifecycle_stage': 'active'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![default-experiment](./img/default-experiment.gif \"default-experiment\")\n",
    "\n",
    "Exploring the Default Experiment\n",
    "\n",
    "Using the MLflow Client’s search_experiments() API to view the Default Experiment\n",
    "In the next step, we’ll create our first experiment and dive into the options that are available for providing metadata information that helps to keep track of related experiments and organize our runs within experiments so that we can effectively compare the results of different parameters for training runs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
